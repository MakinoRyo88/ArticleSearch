import os
import json
import logging
import time
from datetime import datetime
from typing import Dict, List, Optional
from google.cloud import bigquery
import requests
from bs4 import BeautifulSoup
import functions_framework

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ç’°å¢ƒå¤‰æ•°
PROJECT_ID = os.environ.get('PROJECT_ID')
DATASET_ID = os.environ.get('DATASET_ID', 'content_analysis')
ARTICLES_TABLE_ID = os.environ.get('ARTICLES_TABLE_ID', 'articles')
COURSES_TABLE_ID = os.environ.get('COURSES_TABLE_ID', 'courses')
STRAPI_BASE_URL = os.environ.get('STRAPI_BASE_URL')
STRAPI_API_TOKEN = os.environ.get('STRAPI_API_TOKEN')

# BigQueryã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
try:
    client = bigquery.Client(project=PROJECT_ID)
    logger.info(f"BigQuery client initialized for project: {PROJECT_ID}")
except Exception as e:
    logger.error(f"BigQuery client initialization failed: {e}")
    client = None

def make_request_with_retry(url: str, headers: Dict, params: Dict = None, max_retries: int = 5, 
                          timeout: int = 300, backoff_factor: float = 2.0) -> requests.Response:
    """ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãHTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆï¼ˆè†¨å¤§ãªãƒ‡ãƒ¼ã‚¿ã«å¯¾å¿œï¼‰"""
    session = requests.Session()
    # ã‚³ãƒã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ¼ãƒ«ã®è¨­å®š
    adapter = requests.adapters.HTTPAdapter(
        pool_connections=10,
        pool_maxsize=10,
        max_retries=0  # æ‰‹å‹•ã§ãƒªãƒˆãƒ©ã‚¤ã‚’ç®¡ç†
    )
    session.mount('http://', adapter)
    session.mount('https://', adapter)
    
    for attempt in range(max_retries + 1):
        try:
            logger.info(f"Request attempt {attempt + 1}/{max_retries + 1} to {url}")
            
            # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’æ®µéšçš„ã«å¢—åŠ ï¼ˆã‚ˆã‚Šé•·ã‚ã«ï¼‰
            current_timeout = timeout * (1 + attempt * 0.5)
            
            response = session.get(
                url, 
                params=params, 
                headers=headers, 
                timeout=(30, current_timeout),  # (æ¥ç¶šã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ, èª­ã¿å–ã‚Šã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ)
                stream=False
            )
            
            response.raise_for_status()
            logger.info(f"Request successful on attempt {attempt + 1}, size: {len(response.content)} bytes")
            return response
            
        except requests.exceptions.Timeout as e:
            if attempt < max_retries:
                wait_time = backoff_factor ** attempt
                logger.warning(f"Request timeout on attempt {attempt + 1}. Retrying in {wait_time} seconds...")
                time.sleep(wait_time)
            else:
                logger.error(f"Request failed after {max_retries + 1} attempts due to timeout")
                raise
                
        except requests.exceptions.RequestException as e:
            if attempt < max_retries:
                wait_time = backoff_factor ** attempt
                logger.warning(f"Request failed on attempt {attempt + 1}: {str(e)}. Retrying in {wait_time} seconds...")
                time.sleep(wait_time)
            else:
                logger.error(f"Request failed after {max_retries + 1} attempts: {str(e)}")
                raise
        except Exception as e:
            logger.error(f"Unexpected error on attempt {attempt + 1}: {str(e)}")
            if attempt < max_retries:
                time.sleep(backoff_factor ** attempt)
            else:
                raise
    
    raise Exception("Unexpected error in make_request_with_retry")

def extract_text_from_html(html_content: str) -> str:
    """HTMLã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
    if not html_content:
        return ""
    
    try:
        soup = BeautifulSoup(html_content, 'html.parser')
        # ç”»åƒã®altå±æ€§ã‚‚å«ã‚ã‚‹
        for img in soup.find_all('img'):
            if img.get('alt'):
                img.replace_with(f" {img.get('alt')} ")
        
        return soup.get_text(separator=' ', strip=True)
    except Exception as e:
        logger.error(f"HTML parsing error: {e}")
        return str(html_content)

def fetch_strapi_courses_paginated() -> List[Dict]:
    """Strapiã‹ã‚‰è¬›åº§ãƒ‡ãƒ¼ã‚¿ã‚’ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³å¯¾å¿œã§å–å¾—"""
    if not STRAPI_BASE_URL:
        raise ValueError("STRAPI_BASE_URL is not set")
    
    all_courses = []
    page = 1
    page_size = 25  # ãƒšãƒ¼ã‚¸ã‚µã‚¤ã‚ºã‚’å°ã•ãã—ã¦è² è·è»½æ¸›
    
    headers = {}
    if STRAPI_API_TOKEN:
        headers['Authorization'] = f'Bearer {STRAPI_API_TOKEN}'
    
    try:
        while True:
            url = f"{STRAPI_BASE_URL}/api/courses"
            params = {
                'fields[0]': 'slug',
                'fields[1]': 'name',
                'populate': '*',
                'pagination[page]': page,
                'pagination[pageSize]': page_size
            }
            
            logger.info(f"Fetching courses page {page} from: {url}")
            
            response = make_request_with_retry(url, headers, params)
            data = response.json()
            
            courses_data = data.get('data', [])
            if not courses_data:
                logger.info(f"No more courses found on page {page}")
                break
            
            all_courses.extend(courses_data)
            logger.info(f"Page {page}: {len(courses_data)} courses fetched. Total: {len(all_courses)}")
            
            # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³æƒ…å ±ã‚’ãƒã‚§ãƒƒã‚¯
            pagination = data.get('meta', {}).get('pagination', {})
            current_page = pagination.get('page', page)
            page_count = pagination.get('pageCount', 1)
            
            if current_page >= page_count:
                logger.info(f"All pages fetched. Total courses: {len(all_courses)}")
                break
            
            page += 1
            
            # APIãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–ã¨ã—ã¦å°‘ã—å¾…æ©Ÿ
            time.sleep(0.5)
        
        logger.info(f"å–å¾—ã—ãŸè¬›åº§æ•°: {len(all_courses)}")
        return all_courses
        
    except Exception as e:
        logger.error(f"è¬›åº§ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
        raise

def fetch_strapi_articles_paginated() -> List[Dict]:
    """Strapiã‹ã‚‰ã‚³ãƒ©ãƒ ãƒšãƒ¼ã‚¸ãƒ‡ãƒ¼ã‚¿ã‚’ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³å¯¾å¿œã§å–å¾—ï¼ˆè†¨å¤§ãªãƒ‡ãƒ¼ã‚¿ã«å¯¾å¿œï¼‰"""
    if not STRAPI_BASE_URL:
        raise ValueError("STRAPI_BASE_URL is not set")
    
    all_articles = []
    page = 1
    page_size = 50  # ãƒšãƒ¼ã‚¸ã‚µã‚¤ã‚ºã‚’å¢—ã‚„ã—ã¦åŠ¹ç‡åŒ–ï¼ˆ25 â†’ 50ï¼‰
    max_consecutive_failures = 3  # é€£ç¶šå¤±æ•—è¨±å®¹å›æ•°
    consecutive_failures = 0
    
    headers = {}
    if STRAPI_API_TOKEN:
        headers['Authorization'] = f'Bearer {STRAPI_API_TOKEN}'
    
    try:
        while True:
            try:
                url = f"{STRAPI_BASE_URL}/api/colum-pages"
                params = {
                    'populate[0]': 'QandA',
                    'populate[1]': 'article',
                    'populate[2]': 'article.paragraph',
                    'pagination[page]': page,
                    'pagination[pageSize]': page_size
                }
                
                logger.info(f"ğŸ“¥ Fetching articles page {page} (size: {page_size}) from: {url}")
                
                response = make_request_with_retry(url, headers, params, max_retries=5, timeout=300)
                data = response.json()
                
                articles_data = data.get('data', [])
                if not articles_data:
                    logger.info(f"âœ… No more articles found on page {page}. Fetch complete!")
                    break
                
                all_articles.extend(articles_data)
                consecutive_failures = 0  # æˆåŠŸã—ãŸã‚‰ãƒªã‚»ãƒƒãƒˆ
                
                logger.info(f"âœ… Page {page}: {len(articles_data)} articles fetched. Total: {len(all_articles)}")
                
                # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³æƒ…å ±ã‚’ãƒã‚§ãƒƒã‚¯
                pagination = data.get('meta', {}).get('pagination', {})
                current_page = pagination.get('page', page)
                page_count = pagination.get('pageCount', 1)
                total_count = pagination.get('total', 0)
                
                logger.info(f"   Progress: Page {current_page}/{page_count}, Articles: {len(all_articles)}/{total_count}")
                
                if current_page >= page_count:
                    logger.info(f"ğŸ‰ All pages fetched! Total articles: {len(all_articles)}")
                    break
                
                page += 1
                
                # APIãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–ã¨ã—ã¦å°‘ã—å¾…æ©Ÿï¼ˆè†¨å¤§ãªãƒ‡ãƒ¼ã‚¿ãªã®ã§å°‘ã—é•·ã‚ã«ï¼‰
                time.sleep(0.8)
                
            except Exception as page_error:
                consecutive_failures += 1
                logger.error(f"âŒ Error fetching page {page} (attempt {consecutive_failures}/{max_consecutive_failures}): {str(page_error)}")
                
                if consecutive_failures >= max_consecutive_failures:
                    logger.error(f"ğŸ’¥ Too many consecutive failures ({consecutive_failures}). Stopping fetch.")
                    logger.info(f"   Partial data retrieved: {len(all_articles)} articles")
                    break
                
                # å¤±æ•—ã—ãŸãƒšãƒ¼ã‚¸ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦æ¬¡ã¸
                logger.warning(f"âš ï¸  Skipping page {page} and continuing...")
                page += 1
                time.sleep(2)  # å°‘ã—é•·ã‚ã«å¾…æ©Ÿ
        
        logger.info(f"ğŸ“Š Final result: {len(all_articles)} articles fetched")
        return all_articles
        
    except Exception as e:
        logger.error(f"ğŸ’¥ Critical error in fetch_strapi_articles_paginated: {str(e)}")
        if all_articles:
            logger.info(f"   Returning partial data: {len(all_articles)} articles")
            return all_articles
        raise

def process_course_data(raw_courses: List[Dict]) -> List[Dict]:
    """è¬›åº§ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ã—ã¦BigQueryç”¨ã«å¤‰æ›"""
    processed_courses = []
    failed_courses = []
    
    for item in raw_courses:
        try:
            attributes = item.get('attributes', {})
            
            # åŸºæœ¬æƒ…å ±
            course_id = str(item.get('id'))
            slug = attributes.get('slug', '')
            name = attributes.get('name', '')  # nameãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒã‚ã‚‹å ´åˆ
            description = attributes.get('description', '')
            
            # æ—¥æ™‚å¤‰æ›
            created_at = None
            updated_at = None
            
            try:
                if attributes.get('createdAt'):
                    created_at = datetime.fromisoformat(attributes['createdAt'].replace('Z', '+00:00'))
                if attributes.get('updatedAt'):
                    updated_at = datetime.fromisoformat(attributes['updatedAt'].replace('Z', '+00:00'))
            except Exception as e:
                logger.warning(f"Date parsing error for course {course_id}: {e}")
            
            # BigQueryç”¨ãƒ¬ã‚³ãƒ¼ãƒ‰ä½œæˆ
            processed_course = {
                'id': course_id,
                'slug': slug,
                'name': name,  # nameãŒãªã„å ´åˆã¯slugã‚’ä½¿ç”¨
                'description': extract_text_from_html(description),
                'total_articles': 0,  # å¾Œã§é›†è¨ˆ
                'total_pageviews': 0,  # å¾Œã§é›†è¨ˆ
                'created_at': created_at.isoformat() if created_at else None,
                'updated_at': updated_at.isoformat() if updated_at else None,
                'last_synced': datetime.utcnow().isoformat()
            }
            
            processed_courses.append(processed_course)
            
        except Exception as e:
            logger.error(f"è¬›åº§ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚¨ãƒ©ãƒ¼ (ID: {item.get('id')}): {str(e)}")
            failed_courses.append(item.get('id'))
            continue
    
    logger.info(f"å‡¦ç†å®Œäº†ã—ãŸè¬›åº§æ•°: {len(processed_courses)}")
    if failed_courses:
        logger.warning(f"å‡¦ç†å¤±æ•—ã—ãŸè¬›åº§æ•°: {len(failed_courses)}, IDs: {failed_courses}")
    return processed_courses

def process_article_data(raw_articles: List[Dict]) -> List[Dict]:
    """è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ã—ã¦BigQueryç”¨ã«å¤‰æ›ï¼ˆfull_content_htmlç”Ÿæˆã€ã‚¨ãƒ©ãƒ¼ã«å¼·ã„ï¼‰"""
    processed_articles = []
    failed_articles = []
    
    total = len(raw_articles)
    logger.info(f"ğŸ”„ Processing {total} articles...")

    for idx, item in enumerate(raw_articles, 1):
        article_id = str(item.get('id', 'unknown'))
        
        if idx % 50 == 0:
            logger.info(f"   Progress: {idx}/{total} articles processed ({idx/total*100:.1f}%)")
        
        try:
            attributes = item.get('attributes', {})
            
            # åŸºæœ¬æƒ…å ±
            title = attributes.get('POST_TITLE', '')
            link = attributes.get('LINK', '')
            koza_id = str(attributes.get('KOZA_ID', ''))
            
            # HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¨ãƒ—ãƒ¬ãƒ¼ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æ§‹ç¯‰
            html_parts = []
            plain_text_parts = []
            
            # 1. è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ« (h1)
            if title:
                html_parts.append(f"<h1>{title}</h1>")
                plain_text_parts.append(title)
            
            # 2. å†’é ­èª¬æ˜æ–‡ (description)
            description = attributes.get('description', '')
            if description:
                html_parts.append(f"<div class='description'>{description}</div>")
                plain_text_parts.append(extract_text_from_html(description))

            # 3. è¨˜äº‹æœ¬æ–‡ (article)
            for article_section in attributes.get('article', []):
                section_title = article_section.get('title', '')
                if section_title:
                    html_parts.append(f"<h2>{section_title}</h2>")
                    plain_text_parts.append(section_title)
                
                # textãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®å‡¦ç† (å­˜åœ¨ã™ã‚‹å ´åˆ)
                section_text = article_section.get('text')
                if section_text:
                    html_parts.append(section_text)
                    plain_text_parts.append(extract_text_from_html(section_text))

                for paragraph in article_section.get('paragraph', []):
                    subtitle = paragraph.get('subtitle', '')
                    if subtitle:
                        html_parts.append(f"<h3>{subtitle}</h3>")
                        plain_text_parts.append(subtitle)
                    
                    paragraph_text = paragraph.get('text', '')
                    if paragraph_text:
                        html_parts.append(paragraph_text)
                        plain_text_parts.append(extract_text_from_html(paragraph_text))

            # 4. Q&Aã‚»ã‚¯ã‚·ãƒ§ãƒ³
            qanda_list = attributes.get('QandA', [])
            if qanda_list:
                html_parts.append("<h2>Q&A</h2>")
                plain_text_parts.append("Q&A")
                qanda_html = "<dl class='qanda-list'>"
                qanda_plain = []
                for qanda in qanda_list:
                    question = qanda.get('Question', '')
                    answer = qanda.get('Answer', '')
                    if question:
                        qanda_html += f"<dt>{question}</dt>"
                        qanda_plain.append(f"Q: {question}")
                    if answer:
                        qanda_html += f"<dd>{answer}</dd>"
                        qanda_plain.append(f"A: {extract_text_from_html(answer)}")
                qanda_html += "</dl>"
                html_parts.append(qanda_html)
                plain_text_parts.append("\n".join(qanda_plain))

            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’çµåˆ
            full_content_html = "\n".join(html_parts).strip()
            full_content = "\n".join(plain_text_parts).strip()
            qanda_content = "\n".join(qanda_plain) if qanda_list else "" # qanda_contentã‚‚æ›´æ–°

            # æ—¥æ™‚å¤‰æ›
            created_at = None
            updated_at = None
            try:
                if attributes.get('createdAt'):
                    created_at = datetime.fromisoformat(attributes['createdAt'].replace('Z', '+00:00'))
                if attributes.get('updatedAt'):
                    updated_at = datetime.fromisoformat(attributes['updatedAt'].replace('Z', '+00:00'))
            except Exception as e:
                logger.warning(f"Date parsing error for article {article_id}: {e}")
            
            # BigQueryç”¨ãƒ¬ã‚³ãƒ¼ãƒ‰ä½œæˆ
            processed_article = {
                'id': article_id,
                'title': title,
                'link': link,
                'koza_id': koza_id,
                'full_content': full_content,
                'full_content_html': full_content_html,
                'qanda_content': qanda_content,
                'content_type': 'article',
                'pageviews': None,
                'content_embedding': [],
                'embedding_model': None,
                'created_at': created_at.isoformat() if created_at else None,
                'updated_at': updated_at.isoformat() if updated_at else None,
                'last_synced': datetime.utcnow().isoformat()
            }
            processed_articles.append(processed_article)
            
        except Exception as e:
            logger.error(f"âŒ è¨˜äº‹ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚¨ãƒ©ãƒ¼ (ID: {article_id}): {str(e)}", exc_info=False)
            failed_articles.append(article_id)
            # ã‚¨ãƒ©ãƒ¼ã§ã‚‚å‡¦ç†ã‚’ç¶šè¡Œ
            continue
    
    logger.info(f"âœ… å‡¦ç†å®Œäº†ã—ãŸè¨˜äº‹æ•°: {len(processed_articles)}/{total} ({len(processed_articles)/total*100:.1f}%)")
    if failed_articles:
        logger.warning(f"âš ï¸  å‡¦ç†å¤±æ•—ã—ãŸè¨˜äº‹æ•°: {len(failed_articles)}, IDs: {failed_articles[:20]}...")
    
    return processed_articles

def update_course_statistics(courses: List[Dict], articles: List[Dict]):
    """è¬›åº§ã®çµ±è¨ˆæƒ…å ±ï¼ˆè¨˜äº‹æ•°ã€ç·ãƒšãƒ¼ã‚¸ãƒ“ãƒ¥ãƒ¼æ•°ï¼‰ã‚’æ›´æ–°"""
    course_stats = {}
    
    # è¬›åº§åˆ¥ã®è¨˜äº‹æ•°ã‚’é›†è¨ˆ
    for article in articles:
        koza_id = article.get('koza_id', '')
        if koza_id:
            if koza_id not in course_stats:
                course_stats[koza_id] = {'total_articles': 0, 'total_pageviews': 0}
            course_stats[koza_id]['total_articles'] += 1
            # ãƒšãƒ¼ã‚¸ãƒ“ãƒ¥ãƒ¼ãŒã‚ã‚‹å ´åˆã¯åŠ ç®—ï¼ˆå¾Œã§GA4ãƒ‡ãƒ¼ã‚¿ã¨çµ±åˆï¼‰
            if article.get('pageviews'):
                course_stats[koza_id]['total_pageviews'] += article['pageviews']
    
    # è¬›åº§ãƒ‡ãƒ¼ã‚¿ã«çµ±è¨ˆæƒ…å ±ã‚’è¨­å®š
    for course in courses:
        course_id = course['id']
        if course_id in course_stats:
            course['total_articles'] = course_stats[course_id]['total_articles']
            course['total_pageviews'] = course_stats[course_id]['total_pageviews']
    
    logger.info(f"è¬›åº§çµ±è¨ˆæƒ…å ±ã‚’æ›´æ–°: {len(course_stats)}è¬›åº§")

def create_temp_table_from_records(table_id: str, records: List[Dict], record_type: str) -> str:
    """ãƒ¬ã‚³ãƒ¼ãƒ‰ã‹ã‚‰ä¸€æ™‚ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œæˆï¼ˆæ”¹è‰¯ç‰ˆï¼šå¤±æ•—ã‚’æ¸›ã‚‰ã™ï¼‰"""
    if not records:
        return None
    
    # ä¸€æ™‚ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’ç”Ÿæˆ
    timestamp = int(time.time())
    temp_table_name = f"temp_{table_id}_{timestamp}"
    temp_table_id = f"{PROJECT_ID}.{DATASET_ID}.{temp_table_name}"
    
    logger.info(f"ä¸€æ™‚ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆä¸­: {temp_table_id}")
    
    # å…ƒã®ãƒ†ãƒ¼ãƒ–ãƒ«ã®ã‚¹ã‚­ãƒ¼ãƒã‚’å–å¾—
    original_table = client.get_table(f"{PROJECT_ID}.{DATASET_ID}.{table_id}")
    
    # ä¸€æ™‚ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œæˆ
    temp_table = bigquery.Table(temp_table_id, schema=original_table.schema)
    temp_table = client.create_table(temp_table)
    
    # ã‚ˆã‚Šä¿å®ˆçš„ãªãƒãƒƒãƒã‚µã‚¤ã‚ºè¨­å®š
    INITIAL_BATCH_SIZE = 20  # ã•ã‚‰ã«å°ã•ã
    MAX_BATCH_SIZE = 50     # æœ€å¤§å€¤ã‚‚å°ã•ã
    MIN_BATCH_SIZE = 1      # æœ€å°å€¤ã‚’1ã«
    
    current_batch_size = INITIAL_BATCH_SIZE
    total_inserted = 0
    total_failed = 0
    batch_num = 0
    failed_record_ids = []
    
    try:
        i = 0
        while i < len(records):
            batch = records[i:i + current_batch_size]
            batch_num += 1
            
            logger.info(f"ãƒãƒƒãƒ {batch_num} å‡¦ç†ä¸­: {len(batch)}ä»¶ (ã‚µã‚¤ã‚º: {current_batch_size})")
            
            try:
                # ãƒ‡ãƒ¼ã‚¿ã‚’æŒ¿å…¥
                errors = client.insert_rows_json(temp_table, batch)
                
                if errors:
                    logger.error(f"ãƒãƒƒãƒæŒ¿å…¥ã‚¨ãƒ©ãƒ¼: {errors}")
                    
                    # å€‹åˆ¥ãƒ¬ã‚³ãƒ¼ãƒ‰å‡¦ç†ã«åˆ‡ã‚Šæ›¿ãˆ
                    logger.info("å€‹åˆ¥ãƒ¬ã‚³ãƒ¼ãƒ‰å‡¦ç†ã«åˆ‡ã‚Šæ›¿ãˆã¾ã™")
                    success_count = 0
                    for record in batch:
                        try:
                            single_errors = client.insert_rows_json(temp_table, [record])
                            if not single_errors:
                                success_count += 1
                            else:
                                logger.error(f"å€‹åˆ¥ãƒ¬ã‚³ãƒ¼ãƒ‰æŒ¿å…¥ã‚¨ãƒ©ãƒ¼ ID {record.get('id')}: {single_errors}")
                                failed_record_ids.append(record.get('id'))
                                total_failed += 1
                        except Exception as e:
                            logger.error(f"å€‹åˆ¥ãƒ¬ã‚³ãƒ¼ãƒ‰æŒ¿å…¥ä¾‹å¤– ID {record.get('id')}: {str(e)}")
                            failed_record_ids.append(record.get('id'))
                            total_failed += 1
                    
                    total_inserted += success_count
                    logger.info(f"å€‹åˆ¥å‡¦ç†ã§ {success_count}/{len(batch)} ä»¶æˆåŠŸ")
                    
                    # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’æœ€å°ã«èª¿æ•´
                    current_batch_size = MIN_BATCH_SIZE
                else:
                    # æˆåŠŸã—ãŸå ´åˆ
                    total_inserted += len(batch)
                    logger.info(f"ãƒãƒƒãƒ {batch_num} æŒ¿å…¥æˆåŠŸ ({len(batch)}ä»¶)")
                    
                    # æˆåŠŸã—ãŸã‚‰ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å°‘ã—å¢—ã‚„ã™ï¼ˆæœ€å¤§å€¤ã¾ã§ï¼‰
                    if current_batch_size < MAX_BATCH_SIZE:
                        current_batch_size = min(MAX_BATCH_SIZE, current_batch_size + 5)
                
                i += len(batch)  # ãƒãƒƒãƒã‚µã‚¤ã‚ºã§ã¯ãªãã€å®Ÿéš›ã«å‡¦ç†ã—ãŸãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ã§é€²ã‚ã‚‹
                
            except Exception as e:
                error_msg = str(e)
                logger.error(f"ãƒãƒƒãƒå‡¦ç†ã‚¨ãƒ©ãƒ¼: {error_msg}")
                
                # ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰ã‚µã‚¤ã‚ºã‚¨ãƒ©ãƒ¼ã®å ´åˆ
                if "413" in error_msg or "too large" in error_msg.lower() or "payload" in error_msg.lower():
                    if current_batch_size > MIN_BATCH_SIZE:
                        current_batch_size = max(MIN_BATCH_SIZE, current_batch_size // 2)
                        logger.warning(f"ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰ã‚µã‚¤ã‚ºã‚¨ãƒ©ãƒ¼ã€‚ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’ {current_batch_size} ã«å‰Šæ¸›")
                        continue  # åŒã˜ãƒãƒƒãƒã‚’å°ã•ã„ã‚µã‚¤ã‚ºã§å†è©¦è¡Œ
                    else:
                        # æœ€å°ãƒãƒƒãƒã‚µã‚¤ã‚ºã§ã‚‚413ã‚¨ãƒ©ãƒ¼ã®å ´åˆã€å€‹åˆ¥å‡¦ç†
                        logger.warning("å€‹åˆ¥ãƒ¬ã‚³ãƒ¼ãƒ‰å‡¦ç†ã«å¼·åˆ¶åˆ‡ã‚Šæ›¿ãˆï¼ˆå¤§ãã™ãã‚‹ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’åˆ‡ã‚Šè©°ã‚ï¼‰")
                        for record in batch:
                            try:
                                # ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚µã‚¤ã‚ºã‚’å‰Šæ¸›ï¼ˆfull_content_htmlã‚‚å«ã‚€ï¼‰
                                minimal_record = record.copy()
                                if 'full_content' in minimal_record and minimal_record.get('full_content') and len(minimal_record['full_content']) > 50000:
                                    logger.warning(f"  Truncating full_content for ID {record.get('id')}: {len(minimal_record['full_content'])} chars")
                                    minimal_record['full_content'] = minimal_record['full_content'][:50000] + "...[truncated]"
                                if 'full_content_html' in minimal_record and minimal_record.get('full_content_html') and len(minimal_record['full_content_html']) > 50000:
                                    logger.warning(f"  Truncating full_content_html for ID {record.get('id')}: {len(minimal_record['full_content_html'])} chars")
                                    minimal_record['full_content_html'] = minimal_record['full_content_html'][:50000] + "...[truncated]"
                                if 'qanda_content' in minimal_record and minimal_record.get('qanda_content') and len(minimal_record['qanda_content']) > 10000:
                                    minimal_record['qanda_content'] = minimal_record['qanda_content'][:10000] + "...[truncated]"
                                if 'description' in minimal_record and minimal_record.get('description') and len(minimal_record['description']) > 5000:
                                    minimal_record['description'] = minimal_record['description'][:5000] + "...[truncated]"
                                
                                single_errors = client.insert_rows_json(temp_table, [minimal_record])
                                if not single_errors:
                                    total_inserted += 1
                                else:
                                    logger.error(f"æœ€å°åŒ–ãƒ¬ã‚³ãƒ¼ãƒ‰æŒ¿å…¥ã‚¨ãƒ©ãƒ¼ ID {record.get('id')}: {single_errors}")
                                    failed_record_ids.append(record.get('id'))
                                    total_failed += 1
                            except Exception as minimal_error:
                                logger.error(f"æœ€å°åŒ–ãƒ¬ã‚³ãƒ¼ãƒ‰å‡¦ç†ä¾‹å¤– ID {record.get('id')}: {str(minimal_error)}")
                                failed_record_ids.append(record.get('id'))
                                total_failed += 1
                        
                        i += len(batch)
                else:
                    # ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼ - å€‹åˆ¥å‡¦ç†ã‚’è©¦è¡Œ
                    logger.warning("ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼ã«ã‚ˆã‚Šå€‹åˆ¥å‡¦ç†ã«åˆ‡ã‚Šæ›¿ãˆ")
                    for record in batch:
                        try:
                            single_errors = client.insert_rows_json(temp_table, [record])
                            if not single_errors:
                                total_inserted += 1
                            else:
                                failed_record_ids.append(record.get('id'))
                                total_failed += 1
                        except Exception as individual_error:
                            logger.error(f"å€‹åˆ¥å‡¦ç†ä¾‹å¤– ID {record.get('id')}: {str(individual_error)}")
                            failed_record_ids.append(record.get('id'))
                            total_failed += 1
                    
                    i += len(batch)
        
        logger.info(f"ä¸€æ™‚ãƒ†ãƒ¼ãƒ–ãƒ«æŒ¿å…¥å®Œäº†: æˆåŠŸ {total_inserted}/{len(records)} ä»¶, å¤±æ•— {total_failed} ä»¶")
        
        if failed_record_ids:
            logger.warning(f"å¤±æ•—ã—ãŸãƒ¬ã‚³ãƒ¼ãƒ‰ID: {failed_record_ids[:20]}...")  # æœ€åˆã®20ä»¶ã®ã¿è¡¨ç¤º
        
        if total_inserted == 0:
            # ä¸€æ™‚ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å‰Šé™¤
            client.delete_table(temp_table_id)
            raise Exception(f"ä¸€æ™‚ãƒ†ãƒ¼ãƒ–ãƒ«ã¸ã®ãƒ‡ãƒ¼ã‚¿æŒ¿å…¥ãŒå…¨ã¦å¤±æ•—ã—ã¾ã—ãŸ")
        
        return temp_table_name
        
    except Exception as e:
        # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ä¸€æ™‚ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å‰Šé™¤
        try:
            client.delete_table(temp_table_id)
        except:
            pass
        raise Exception(f"ä¸€æ™‚ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆã‚¨ãƒ©ãƒ¼: {str(e)}")

def check_streaming_buffer(table_id: str) -> bool:
    """Streaming Bufferã®çŠ¶æ…‹ã‚’ãƒã‚§ãƒƒã‚¯"""
    try:
        full_table_id = f"{PROJECT_ID}.{DATASET_ID}.{table_id}"
        table = client.get_table(full_table_id)
        
        # Streaming Bufferã®æƒ…å ±ã‚’ç¢ºèª
        if hasattr(table, 'streaming_buffer') and table.streaming_buffer:
            estimated_rows = table.streaming_buffer.estimated_rows
            oldest_entry_time = table.streaming_buffer.oldest_entry_time
            logger.warning(f"Streaming Bufferæ¤œå‡º: {estimated_rows}è¡Œ, æœ€å¤ã‚¨ãƒ³ãƒˆãƒª: {oldest_entry_time}")
            return True
        return False
    except Exception as e:
        logger.warning(f"Streaming Bufferç¢ºèªã‚¨ãƒ©ãƒ¼: {str(e)}")
        return False

def wait_for_streaming_buffer_clear(table_id: str, max_wait_minutes: int = 5) -> bool:
    """Streaming BufferãŒã‚¯ãƒªã‚¢ã•ã‚Œã‚‹ã¾ã§å¾…æ©Ÿï¼ˆçŸ­æ™‚é–“ï¼‰"""
    import time
    
    full_table_id = f"{PROJECT_ID}.{DATASET_ID}.{table_id}"
    wait_seconds = 0
    max_wait_seconds = max_wait_minutes * 60
    check_interval = 30  # 30ç§’é–“éš”ã§ãƒã‚§ãƒƒã‚¯
    
    logger.info(f"Streaming Bufferã‚¯ãƒªã‚¢å¾…æ©Ÿé–‹å§‹ï¼ˆæœ€å¤§{max_wait_minutes}åˆ†ï¼‰")
    
    while wait_seconds < max_wait_seconds:
        try:
            table = client.get_table(full_table_id)
            
            if not hasattr(table, 'streaming_buffer') or not table.streaming_buffer:
                logger.info(f"Streaming BufferãŒã‚¯ãƒªã‚¢ã•ã‚Œã¾ã—ãŸï¼ˆ{wait_seconds}ç§’å¾Œï¼‰")
                return True
            
            if table.streaming_buffer:
                estimated_rows = table.streaming_buffer.estimated_rows or 0
                logger.info(f"Streaming Bufferæ®‹å­˜: {estimated_rows}è¡Œ - {wait_seconds}/{max_wait_seconds}ç§’çµŒé")
            
            time.sleep(check_interval)
            wait_seconds += check_interval
            
        except Exception as e:
            logger.warning(f"Streaming Bufferç¢ºèªã‚¨ãƒ©ãƒ¼: {str(e)}")
            time.sleep(check_interval)
            wait_seconds += check_interval
    
    logger.warning(f"Streaming Bufferã‚¯ãƒªã‚¢å¾…æ©Ÿã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆ{max_wait_minutes}åˆ†ï¼‰")
    return False

def force_update_existing_records(table_id: str, records: List[Dict], record_type: str):
    """æ—¢å­˜ãƒ¬ã‚³ãƒ¼ãƒ‰ã®å¼·åˆ¶æ›´æ–°ï¼ˆDELETE + INSERTæ–¹å¼ï¼‰"""
    if not records:
        return
    
    full_table_id = f"{PROJECT_ID}.{DATASET_ID}.{table_id}"
    
    try:
        # æ›´æ–°å¯¾è±¡ã®IDãƒªã‚¹ãƒˆã‚’ä½œæˆ
        record_ids = [record['id'] for record in records]
        ids_string = "', '".join(record_ids)
        
        logger.info(f"æ—¢å­˜{record_type}ãƒ¬ã‚³ãƒ¼ãƒ‰ {len(record_ids)}ä»¶ã®å¼·åˆ¶æ›´æ–°ã‚’é–‹å§‹")
        
        # 1. æ—¢å­˜ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’å‰Šé™¤
        delete_query = f"""
        DELETE FROM `{full_table_id}`
        WHERE id IN ('{ids_string}')
        """
        
        logger.info(f"æ—¢å­˜ãƒ¬ã‚³ãƒ¼ãƒ‰å‰Šé™¤ä¸­...")
        delete_job = client.query(delete_query)
        delete_result = delete_job.result()
        
        if hasattr(delete_job, 'num_dml_affected_rows'):
            deleted_rows = delete_job.num_dml_affected_rows
            logger.info(f"å‰Šé™¤å®Œäº†: {deleted_rows}è¡Œ")
        
        # 2. æ–°ã—ã„ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’æŒ¿å…¥
        logger.info(f"æ–°ã—ã„ãƒ¬ã‚³ãƒ¼ãƒ‰æŒ¿å…¥ä¸­...")
        table = client.get_table(full_table_id)
        
        # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’è¨­å®š
        BATCH_SIZE = 50  # å®‰å…¨ã®ãŸã‚å°ã•ã‚ã«è¨­å®š
        total_inserted = 0
        
        for i in range(0, len(records), BATCH_SIZE):
            batch = records[i:i + BATCH_SIZE]
            batch_num = (i // BATCH_SIZE) + 1
            total_batches = (len(records) + BATCH_SIZE - 1) // BATCH_SIZE
            
            logger.info(f"æ›´æ–°ãƒãƒƒãƒ {batch_num}/{total_batches} ã‚’å‡¦ç†ä¸­... ({len(batch)}ä»¶)")
            
            try:
                errors = client.insert_rows_json(table, batch)
                if errors:
                    logger.error(f"ãƒãƒƒãƒæŒ¿å…¥ã‚¨ãƒ©ãƒ¼: {errors}")
                    # å€‹åˆ¥ãƒ¬ã‚³ãƒ¼ãƒ‰ã§å†è©¦è¡Œ
                    for record in batch:
                        try:
                            single_errors = client.insert_rows_json(table, [record])
                            if not single_errors:
                                total_inserted += 1
                            else:
                                logger.error(f"å€‹åˆ¥ãƒ¬ã‚³ãƒ¼ãƒ‰æŒ¿å…¥ã‚¨ãƒ©ãƒ¼ ID {record.get('id')}: {single_errors}")
                        except Exception as e:
                            logger.error(f"å€‹åˆ¥ãƒ¬ã‚³ãƒ¼ãƒ‰æŒ¿å…¥ä¾‹å¤– ID {record.get('id')}: {str(e)}")
                else:
                    total_inserted += len(batch)
                    logger.info(f"ãƒãƒƒãƒ {batch_num} æŒ¿å…¥æˆåŠŸ ({len(batch)}ä»¶)")
            except Exception as e:
                logger.error(f"ãƒãƒƒãƒå‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        logger.info(f"å¼·åˆ¶æ›´æ–°å®Œäº†: {total_inserted}ä»¶ã®{record_type}ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’æ›´æ–°ã—ã¾ã—ãŸ")
        
    except Exception as e:
        logger.error(f"å¼·åˆ¶æ›´æ–°ã‚¨ãƒ©ãƒ¼: {str(e)}")
        raise

def upsert_to_bigquery_with_fallback(table_id: str, records: List[Dict], record_type: str):
    """æ”¹è‰¯ã•ã‚ŒãŸStreaming Bufferã‚’è€ƒæ…®ã—ãŸãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä»˜ãUPSERT"""
    if not records:
        logger.info(f"ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹{record_type}ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        return
    
    if not client:
        raise Exception("BigQuery client is not initialized")
    
    full_table_id = f"{PROJECT_ID}.{DATASET_ID}.{table_id}"
    
    # Streaming Bufferã®ç¢ºèª
    has_streaming_buffer = check_streaming_buffer(table_id)
    
    if has_streaming_buffer:
        logger.warning(f"Streaming BufferãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚")
        
        # çŸ­æ™‚é–“å¾…æ©Ÿã—ã¦Streaming Bufferã‚¯ãƒªã‚¢ã‚’è©¦è¡Œ
        if wait_for_streaming_buffer_clear(table_id, max_wait_minutes=3):
            logger.info("Streaming BufferãŒã‚¯ãƒªã‚¢ã•ã‚Œã¾ã—ãŸã€‚MERGEæ–‡ã§å‡¦ç†ã—ã¾ã™ã€‚")
            try:
                upsert_to_bigquery_merge(table_id, records, record_type)
                return
            except Exception as e:
                if "streaming buffer" in str(e).lower():
                    logger.warning(f"MERGEæ–‡ã§Streaming Bufferã‚¨ãƒ©ãƒ¼ãŒå†ç™ºã€‚å¼·åˆ¶æ›´æ–°ãƒ¢ãƒ¼ãƒ‰ã«åˆ‡ã‚Šæ›¿ãˆã¾ã™ã€‚")
                else:
                    raise
        
        # Streaming BufferãŒã‚¯ãƒªã‚¢ã•ã‚Œãªã„å ´åˆã¯å¼·åˆ¶æ›´æ–°ãƒ¢ãƒ¼ãƒ‰
        logger.info("å¼·åˆ¶æ›´æ–°ãƒ¢ãƒ¼ãƒ‰ï¼ˆDELETE + INSERTï¼‰ã§å‡¦ç†ã—ã¾ã™")
        
        # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®IDã‚’å–å¾—
        existing_ids_query = f"SELECT id FROM `{full_table_id}`"
        existing_ids = set()
        
        try:
            query_job = client.query(existing_ids_query)
            for row in query_job:
                existing_ids.add(row.id)
            logger.info(f"æ—¢å­˜{record_type}IDæ•°: {len(existing_ids)}")
        except Exception as e:
            logger.warning(f"æ—¢å­˜IDå–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        # æ–°è¦ã¨æ›´æ–°å¯¾è±¡ã«åˆ†é¡
        new_records = []
        update_records = []
        
        for record in records:
            if record['id'] in existing_ids:
                update_records.append(record)
            else:
                new_records.append(record)
        
        logger.info(f"{record_type}ãƒ‡ãƒ¼ã‚¿åˆ†æ: æ–°è¦{len(new_records)}ä»¶, æ›´æ–°å¯¾è±¡{len(update_records)}ä»¶")
        
        # æ–°è¦ãƒ‡ãƒ¼ã‚¿ã®æŒ¿å…¥
        if new_records:
            logger.info(f"æ–°è¦{record_type}ãƒ‡ãƒ¼ã‚¿ã‚’æŒ¿å…¥ä¸­...")
            table = client.get_table(full_table_id)
            
            BATCH_SIZE = 50
            total_inserted = 0
            
            for i in range(0, len(new_records), BATCH_SIZE):
                batch = new_records[i:i + BATCH_SIZE]
                try:
                    errors = client.insert_rows_json(table, batch)
                    if not errors:
                        total_inserted += len(batch)
                    else:
                        logger.error(f"æ–°è¦ãƒ‡ãƒ¼ã‚¿æŒ¿å…¥ã‚¨ãƒ©ãƒ¼: {errors}")
                except Exception as e:
                    logger.error(f"æ–°è¦ãƒ‡ãƒ¼ã‚¿æŒ¿å…¥ä¾‹å¤–: {str(e)}")
            
            logger.info(f"æ–°è¦{record_type}ãƒ‡ãƒ¼ã‚¿ {total_inserted}ä»¶ã‚’æŒ¿å…¥ã—ã¾ã—ãŸ")
        
        # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®å¼·åˆ¶æ›´æ–°
        if update_records:
            force_update_existing_records(table_id, update_records, record_type)
        
    else:
        # Streaming BufferãŒãªã„å ´åˆã¯MERGEæ–‡ã‚’ä½¿ç”¨
        try:
            upsert_to_bigquery_merge(table_id, records, record_type)
        except Exception as e:
            if "streaming buffer" in str(e).lower():
                logger.warning(f"MERGEæ–‡ã§Streaming Bufferã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã€‚å¼·åˆ¶æ›´æ–°ãƒ¢ãƒ¼ãƒ‰ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: {str(e)}")
                # å†å¸°çš„ã«å‘¼ã³å‡ºã™ã®ã§ã¯ãªãã€å¼·åˆ¶æ›´æ–°ã‚’ç›´æ¥å®Ÿè¡Œ
                force_update_existing_records(table_id, records, record_type)
            else:
                raise

def upsert_to_bigquery_merge(table_id: str, records: List[Dict], record_type: str):
    """æ”¹è‰¯ã•ã‚ŒãŸMERGEæ–‡ã‚’ä½¿ç”¨ã—ãŸUPSERTå‡¦ç†"""
    temp_table_name = None
    
    try:
        full_table_id = f"{PROJECT_ID}.{DATASET_ID}.{table_id}"
        
        # ä¸€æ™‚ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œæˆã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’æŒ¿å…¥
        temp_table_name = create_temp_table_from_records(table_id, records, record_type)
        if not temp_table_name:
            logger.info(f"å‡¦ç†ã™ã‚‹{record_type}ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        temp_table_id = f"{PROJECT_ID}.{DATASET_ID}.{temp_table_name}"
        
        # ãƒ†ãƒ¼ãƒ–ãƒ«ã®ç¨®é¡ã«å¿œã˜ã¦MERGEæ–‡ã‚’æ§‹ç¯‰
        if table_id == COURSES_TABLE_ID:
            merge_query = f"""
            MERGE `{full_table_id}` T
            USING `{temp_table_id}` S
            ON T.id = S.id
            WHEN MATCHED THEN
              UPDATE SET
                slug = S.slug,
                name = S.name,
                description = S.description,
                total_articles = S.total_articles,
                total_pageviews = S.total_pageviews,
                updated_at = S.updated_at,
                last_synced = S.last_synced
            WHEN NOT MATCHED THEN
              INSERT (id, slug, name, description, total_articles, total_pageviews, created_at, updated_at, last_synced)
              VALUES (S.id, S.slug, S.name, S.description, S.total_articles, S.total_pageviews, S.created_at, S.updated_at, S.last_synced)
            """
        elif table_id == ARTICLES_TABLE_ID:
            # è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã®MERGEæ–‡ã‚’æ”¹è‰¯ï¼ˆfull_content_htmlã®ã¿ä½¿ç”¨ï¼‰
            merge_query = f"""
            MERGE `{full_table_id}` T
            USING `{temp_table_id}` S
            ON T.id = S.id
            WHEN MATCHED THEN
              UPDATE SET
                title = S.title,
                link = S.link,
                koza_id = S.koza_id,
                full_content_html = S.full_content_html,
                qanda_content = S.qanda_content,
                content_type = S.content_type,
                pageviews = COALESCE(S.pageviews, T.pageviews),
                content_embedding = CASE 
                  WHEN S.content_embedding IS NOT NULL AND ARRAY_LENGTH(S.content_embedding) > 0 
                  THEN S.content_embedding 
                  ELSE T.content_embedding 
                END,
                embedding_model = COALESCE(S.embedding_model, T.embedding_model),
                updated_at = S.updated_at,
                last_synced = S.last_synced
            WHEN NOT MATCHED THEN
              INSERT (id, title, link, koza_id, full_content_html, qanda_content, content_type, pageviews, content_embedding, embedding_model, created_at, updated_at, last_synced)
              VALUES (S.id, S.title, S.link, S.koza_id, S.full_content_html, S.qanda_content, S.content_type, S.pageviews, S.content_embedding, S.embedding_model, S.created_at, S.updated_at, S.last_synced)
            """
        else:
            raise ValueError(f"ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ãƒ†ãƒ¼ãƒ–ãƒ«: {table_id}")
        
        logger.info(f"{record_type}ãƒ‡ãƒ¼ã‚¿ã®MERGEå‡¦ç†ã‚’å®Ÿè¡Œä¸­...")
        
        # MERGEæ–‡ã‚’å®Ÿè¡Œ
        query_job = client.query(merge_query)
        result = query_job.result()  # å®Ÿè¡Œå®Œäº†ã‚’å¾…æ©Ÿ
        
        # çµæœã‚’ç¢ºèª
        if hasattr(query_job, 'num_dml_affected_rows') and query_job.num_dml_affected_rows is not None:
            affected_rows = query_job.num_dml_affected_rows
            logger.info(f"MERGEå‡¦ç†å®Œäº†: {affected_rows}è¡ŒãŒå½±éŸ¿ã‚’å—ã‘ã¾ã—ãŸ")
        else:
            logger.info(f"MERGEå‡¦ç†å®Œäº†: {record_type}ãƒ‡ãƒ¼ã‚¿ãŒæ­£å¸¸ã«åŒæœŸã•ã‚Œã¾ã—ãŸ")
        
        # å®Ÿéš›ã®æ›´æ–°ãƒ»æŒ¿å…¥ä»¶æ•°ã‚’ã‚ˆã‚Šè©³ç´°ã«ç¢ºèª
        check_query = f"""
        SELECT 
          COUNT(*) as total_count,
          COUNT(CASE WHEN last_synced >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 5 MINUTE) THEN 1 END) as recent_sync_count
        FROM `{full_table_id}`
        WHERE id IN (SELECT id FROM `{temp_table_id}`)
        """
        
        check_job = client.query(check_query)
        for row in check_job:
            logger.info(f"åŒæœŸç¢ºèª: å¯¾è±¡{row.total_count}ä»¶ä¸­{row.recent_sync_count}ä»¶ãŒæœ€è¿‘åŒæœŸã•ã‚Œã¾ã—ãŸ")
        
        logger.info(f"BigQueryã«{len(records)}ä»¶ã®{record_type}ãƒ‡ãƒ¼ã‚¿ã‚’æ­£å¸¸ã«UPSERTã—ã¾ã—ãŸ")
        
    finally:
        # ä¸€æ™‚ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å‰Šé™¤
        if temp_table_name:
            try:
                temp_table_id = f"{PROJECT_ID}.{DATASET_ID}.{temp_table_name}"
                client.delete_table(temp_table_id)
                logger.info(f"ä¸€æ™‚ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å‰Šé™¤ã—ã¾ã—ãŸ: {temp_table_name}")
            except Exception as e:
                logger.warning(f"ä¸€æ™‚ãƒ†ãƒ¼ãƒ–ãƒ«ã®å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {str(e)}")

# ä¸‹ä½äº’æ›ã®ãŸã‚ã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹
def upsert_to_bigquery(table_id: str, records: List[Dict], record_type: str):
    """BigQueryã«ãƒ‡ãƒ¼ã‚¿ã‚’UPSERTï¼ˆStreaming Bufferå¯¾å¿œï¼‰"""
    return upsert_to_bigquery_with_fallback(table_id, records, record_type)

@functions_framework.http
def sync_strapi_data(request):
    """Cloud Functions HTTPã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ - è¬›åº§ã¨è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã®åŒæœŸ"""
    try:
        logger.info("=== Strapi ãƒ‡ãƒ¼ã‚¿åŒæœŸé–‹å§‹ï¼ˆè¬›åº§ãƒ»è¨˜äº‹ï¼‰ ===")
        logger.info(f"Request method: {request.method}")
        logger.info(f"Request path: {request.path}")
        
        # ç’°å¢ƒå¤‰æ•°ãƒã‚§ãƒƒã‚¯
        if not PROJECT_ID:
            raise ValueError("PROJECT_ID environment variable is required")
        if not STRAPI_BASE_URL:
            raise ValueError("STRAPI_BASE_URL environment variable is required")
        
        logger.info(f"Project ID: {PROJECT_ID}")
        logger.info(f"Dataset ID: {DATASET_ID}")
        logger.info(f"Articles Table ID: {ARTICLES_TABLE_ID}")
        logger.info(f"Courses Table ID: {COURSES_TABLE_ID}")
        logger.info(f"Strapi URL: {STRAPI_BASE_URL}")
        
        # 1. è¬›åº§ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ»å‡¦ç†
        logger.info("=" * 60)
        logger.info("ğŸ“š PHASE 1: è¬›åº§ãƒ‡ãƒ¼ã‚¿å‡¦ç†")
        logger.info("=" * 60)
        raw_courses = fetch_strapi_courses_paginated()
        processed_courses = process_course_data(raw_courses)
        
        # 2. è¨˜äº‹ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ»å‡¦ç†
        logger.info("=" * 60)
        logger.info("ğŸ“ PHASE 2: è¨˜äº‹ãƒ‡ãƒ¼ã‚¿å‡¦ç†")
        logger.info("=" * 60)
        raw_articles = fetch_strapi_articles_paginated()
        processed_articles = process_article_data(raw_articles)
        
        # 3. è¬›åº§çµ±è¨ˆæƒ…å ±ã®æ›´æ–°
        logger.info("=" * 60)
        logger.info("ğŸ“Š PHASE 3: è¬›åº§çµ±è¨ˆæƒ…å ±æ›´æ–°")
        logger.info("=" * 60)
        update_course_statistics(processed_courses, processed_articles)
        
        # 4. BigQueryã«ä¿å­˜ï¼ˆè¬›åº§ãƒ‡ãƒ¼ã‚¿ï¼‰
        logger.info("=" * 60)
        logger.info("ğŸ’¾ PHASE 4: è¬›åº§ãƒ‡ãƒ¼ã‚¿ä¿å­˜")
        logger.info("=" * 60)
        upsert_to_bigquery(COURSES_TABLE_ID, processed_courses, "è¬›åº§")
        
        # 5. BigQueryã«ä¿å­˜ï¼ˆè¨˜äº‹ãƒ‡ãƒ¼ã‚¿ï¼‰
        logger.info("=" * 60)
        logger.info("ğŸ’¾ PHASE 5: è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ä¿å­˜ï¼ˆfull_content_htmlå«ã‚€ï¼‰")
        logger.info("=" * 60)
        upsert_to_bigquery(ARTICLES_TABLE_ID, processed_articles, "è¨˜äº‹")
        
        result = {
            'status': 'success',
            'message': f'ğŸ‰ è¬›åº§{len(processed_courses)}ä»¶ã€è¨˜äº‹{len(processed_articles)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’åŒæœŸã—ã¾ã—ãŸï¼ˆfull_content_htmlå«ã‚€ï¼‰',
            'courses_count': len(processed_courses),
            'articles_count': len(processed_articles),
            'raw_articles_fetched': len(raw_articles),
            'timestamp': datetime.utcnow().isoformat(),
            'project_id': PROJECT_ID,
            'dataset_id': DATASET_ID,
            'courses_table_id': COURSES_TABLE_ID,
            'articles_table_id': ARTICLES_TABLE_ID
        }
        
        logger.info("=" * 60)
        logger.info("ğŸ‰ åŒæœŸå®Œäº†ï¼")
        logger.info("=" * 60)
        logger.info(f"çµæœ: {result}")
        return result, 200
        
    except Exception as e:
        logger.error(f"=== åŒæœŸå‡¦ç†ã‚¨ãƒ©ãƒ¼ ===: {str(e)}", exc_info=True)
        error_result = {
            'status': 'error',
            'message': str(e),
            'timestamp': datetime.utcnow().isoformat()
        }
        return error_result, 500

@functions_framework.http
def hello_world(request):
    """ãƒ†ã‚¹ãƒˆç”¨ã®ç°¡å˜ãªã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    return {'message': 'Hello from Cloud Functions!', 'timestamp': datetime.utcnow().isoformat()}, 200